{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transfer Learning via Pytorch Framework (custom images)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports here\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sb\n",
    "import cv2\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets, transforms, models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = 'flowers'\n",
    "train_dir = data_dir + '/train'\n",
    "valid_dir = data_dir + '/valid'\n",
    "test_dir = data_dir + '/test'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define transforms for the training, validation, and testing sets\n",
    "training_transforms = transforms.Compose([transforms.RandomRotation(30),\n",
    "                                          transforms.RandomResizedCrop(96),\n",
    "                                          transforms.RandomHorizontalFlip(),\n",
    "                                          transforms.ToTensor(),\n",
    "                                          transforms.Normalize([0.485, 0.456, 0.406], \n",
    "                                                               [0.229, 0.224, 0.225])])\n",
    "\n",
    "validation_transforms = transforms.Compose([transforms.Resize(256),\n",
    "                                            transforms.CenterCrop(96),\n",
    "                                            transforms.ToTensor(),\n",
    "                                            transforms.Normalize([0.485, 0.456, 0.406], \n",
    "                                                                 [0.229, 0.224, 0.225])])\n",
    "\n",
    "testing_transforms = transforms.Compose([transforms.Resize(256),\n",
    "                                         transforms.CenterCrop(96),\n",
    "                                         transforms.ToTensor(),\n",
    "                                         transforms.Normalize([0.485, 0.456, 0.406], \n",
    "                                                              [0.229, 0.224, 0.225])])\n",
    "\n",
    "# TODO: Load the datasets with ImageFolder\n",
    "training_dataset = datasets.ImageFolder(train_dir, transform=training_transforms)\n",
    "validation_dataset = datasets.ImageFolder(valid_dir, transform=validation_transforms)\n",
    "testing_dataset = datasets.ImageFolder(test_dir, transform=testing_transforms)\n",
    "\n",
    "# TODO: Using the image datasets and the trainforms, define the dataloaders\n",
    "train_loader = torch.utils.data.DataLoader(training_dataset, batch_size=64, shuffle=True)\n",
    "validate_loader = torch.utils.data.DataLoader(validation_dataset, batch_size=32)\n",
    "test_loader = torch.utils.data.DataLoader(testing_dataset, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optional step, unless u need to print result.\n",
    "import json\n",
    "\n",
    "with open('flower_to_name.json', 'r') as f:\n",
    "    flower_to_name = json.load(f)\n",
    "    \n",
    "print(len(flower_to_name)) \n",
    "print(flower_to_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build and train your network\n",
    "# Transfer Learning\n",
    "model = models.vgg16(pretrained=True)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Freeze pretrained model parameters to avoid backpropogating through them\n",
    "for parameter in model.parameters():\n",
    "    parameter.requires_grad = False\n",
    "\n",
    "\n",
    "from collections import OrderedDict\n",
    "\n",
    "# Build custom classifier \n",
    "classifier = nn.Sequential(OrderedDict([('fc1', nn.Linear(25088, 5000)), # Noel in_features=25088,but 5000, no idea.\n",
    "                                        ('relu', nn.ReLU()),\n",
    "                                        ('drop', nn.Dropout(p=0.5)),\n",
    "                                        ('fc2', nn.Linear(5000, 102)),\n",
    "                                        ('output', nn.LogSoftmax(dim=1))]))\n",
    "\n",
    "model.classifier = classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for the validation pass\n",
    "def validation(model, validateloader, criterion):\n",
    "    \n",
    "    val_loss = 0\n",
    "    accuracy = 0\n",
    "    \n",
    "    for images, labels in iter(validateloader):\n",
    "\n",
    "        images, labels = images.to('cuda'), labels.to('cuda')\n",
    "\n",
    "        output = model.forward(images)\n",
    "        val_loss += criterion(output, labels).item()\n",
    "\n",
    "        probabilities = torch.exp(output)\n",
    "        \n",
    "        equality = (labels.data == probabilities.max(dim=1)[1])\n",
    "        accuracy += equality.type(torch.FloatTensor).mean()\n",
    "    \n",
    "    return val_loss, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss function and gradient descent\n",
    "\n",
    "criterion = nn.NLLLoss()\n",
    "\n",
    "optimizer = optim.Adam(model.classifier.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the classifier\n",
    "\n",
    "from workspace_utils import active_session\n",
    "\n",
    "def train_classifier():\n",
    "\n",
    "    #with active_session():\n",
    "\n",
    "    epochs = 15\n",
    "    steps = 0\n",
    "    print_every = 40\n",
    "\n",
    "    model.to('cuda')\n",
    "\n",
    "    for e in range(epochs):\n",
    "\n",
    "        model.train()\n",
    "\n",
    "        running_loss = 0\n",
    "\n",
    "        for images, labels in iter(train_loader):\n",
    "\n",
    "            steps += 1\n",
    "\n",
    "            images, labels = images.to('cuda'), labels.to('cuda')\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            output = model.forward(images)\n",
    "            loss = criterion(output, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "\n",
    "            if steps % print_every == 0:\n",
    "\n",
    "                model.eval()\n",
    "\n",
    "                # Turn off gradients for validation, saves memory and computations\n",
    "                with torch.no_grad():\n",
    "                    validation_loss, accuracy = validation(model, validate_loader, criterion)\n",
    "\n",
    "                print(\"Epoch: {}/{}.. \".format(e+1, epochs),\n",
    "                      \"Training Loss: {:.3f}.. \".format(running_loss/print_every),\n",
    "                      \"Validation Loss: {:.3f}.. \".format(validation_loss/len(validate_loader)),\n",
    "                      \"Validation Accuracy: {:.3f}\".format(accuracy/len(validate_loader)))\n",
    "\n",
    "                running_loss = 0\n",
    "                model.train()\n",
    "                    \n",
    "train_classifier()                    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing your network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_accuracy(model, test_loader):\n",
    "\n",
    "    # Do validation on the test set\n",
    "    model.eval()\n",
    "    model.to('cuda')\n",
    "\n",
    "    with torch.no_grad():\n",
    "    \n",
    "        accuracy = 0\n",
    "    \n",
    "        for images, labels in iter(test_loader):\n",
    "    \n",
    "            images, labels = images.to('cuda'), labels.to('cuda')\n",
    "    \n",
    "            output = model.forward(images)\n",
    "\n",
    "            probabilities = torch.exp(output)\n",
    "        \n",
    "            equality = (labels.data == probabilities.max(dim=1)[1])\n",
    "        \n",
    "            accuracy += equality.type(torch.FloatTensor).mean()\n",
    "        \n",
    "        print(\"Test Accuracy: {}\".format(accuracy/len(test_loader)))    \n",
    "        \n",
    "        \n",
    "test_accuracy(model, test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save the checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the checkpoint\n",
    "\n",
    "def save_checkpoint(model):\n",
    "\n",
    "    model.class_to_idx = training_dataset.class_to_idx\n",
    "\n",
    "    checkpoint = {'arch': \"vgg16\",\n",
    "                  'class_to_idx': model.class_to_idx,\n",
    "                  'model_state_dict': model.state_dict()\n",
    "                 }\n",
    "\n",
    "    torch.save(checkpoint, 'checkpoint.pth')\n",
    "    \n",
    "save_checkpoint(model)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "\n",
    "# Function that loads a checkpoint and rebuilds the model\n",
    "\n",
    "def load_checkpoint(filepath):\n",
    "    \n",
    "    checkpoint = torch.load(filepath)\n",
    "    \n",
    "    if checkpoint['arch'] == 'vgg16':\n",
    "        \n",
    "        model = models.vgg16(pretrained=True)\n",
    "        \n",
    "        for param in model.parameters():\n",
    "            param.requires_grad = False\n",
    "    else:\n",
    "        print(\"Architecture not recognized.\")\n",
    "    \n",
    "    model.class_to_idx = checkpoint['class_to_idx']\n",
    "    \n",
    "    classifier = nn.Sequential(OrderedDict([('fc1', nn.Linear(25088, 5000)),\n",
    "                                            ('relu', nn.ReLU()),\n",
    "                                            ('drop', nn.Dropout(p=0.5)),\n",
    "                                            ('fc2', nn.Linear(5000, 102)),\n",
    "                                            ('output', nn.LogSoftmax(dim=1))]))\n",
    "\n",
    "    model.classifier = classifier\n",
    "    \n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    \n",
    "    return model\n",
    "\n",
    "model = load_checkpoint('checkpoint.pth')\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference for classification\n",
    "\n",
    "```python\n",
    "probs, classes = predict(image_path, model)\n",
    "print(probs)\n",
    "print(classes)\n",
    "> [ 0.01558163  0.01541934  0.01452626  0.01443549  0.01407339]\n",
    "> ['70', '3', '45', '62', '55']\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "\n",
    "def process_image(image_path):\n",
    "    ''' Scales, crops, and normalizes a PIL image for a PyTorch model,\n",
    "        returns an Numpy array\n",
    "    '''\n",
    "    \n",
    "    # Process a PIL image for use in a PyTorch model\n",
    "    \n",
    "    pil_image = Image.open(image_path)\n",
    "    \n",
    "    # Resize\n",
    "    if pil_image.size[0] > pil_image.size[1]:\n",
    "        pil_image.thumbnail((5000, 256))\n",
    "    else:\n",
    "        pil_image.thumbnail((256, 5000))\n",
    "        \n",
    "    # Crop \n",
    "    left_margin = (pil_image.width-224)/2\n",
    "    bottom_margin = (pil_image.height-224)/2\n",
    "    right_margin = left_margin + 224\n",
    "    top_margin = bottom_margin + 224\n",
    "    \n",
    "    pil_image = pil_image.crop((left_margin, bottom_margin, right_margin, top_margin))\n",
    "    \n",
    "    # Normalize\n",
    "    np_image = np.array(pil_image)/255\n",
    "    mean = np.array([0.485, 0.456, 0.406])\n",
    "    std = np.array([0.229, 0.224, 0.225])\n",
    "    np_image = (np_image - mean) / std\n",
    "    \n",
    "    # PyTorch expects the color channel to be the first dimension but it's the third dimension in the PIL image and Numpy array\n",
    "    # Color channel needs to be first; retain the order of the other two dimensions.\n",
    "    np_image = np_image.transpose((2, 0, 1))\n",
    "    \n",
    "    return np_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def imshow(image, ax=None, title=None):\n",
    "    if ax is None:\n",
    "        fig, ax = plt.subplots()\n",
    "    \n",
    "    # PyTorch tensors assume the color channel is the first dimension\n",
    "    # but matplotlib assumes is the third dimension\n",
    "    image = image.transpose((1, 2, 0))\n",
    "    \n",
    "    # Undo preprocessing\n",
    "    mean = np.array([0.485, 0.456, 0.406])\n",
    "    std = np.array([0.229, 0.224, 0.225])\n",
    "    image = std * image + mean\n",
    "    \n",
    "    if title is not None:\n",
    "        ax.set_title(title)\n",
    "    \n",
    "    # Image needs to be clipped between 0 and 1 or it looks like noise when displayed\n",
    "    image = np.clip(image, 0, 1)\n",
    "    \n",
    "    ax.imshow(image)\n",
    "    \n",
    "    return ax\n",
    "\n",
    "image = process_image('flowers/test/1/image_06743.jpg')\n",
    "imshow(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement the code to predict the class from an image file\n",
    "\n",
    "def predict(image_path, model, topk=5):\n",
    "    ''' Predict the class (or classes) of an image using a trained deep learning model.\n",
    "    '''\n",
    "    \n",
    "    image = process_image(image_path)\n",
    "    \n",
    "    # Convert image to PyTorch tensor first\n",
    "    image = torch.from_numpy(image).type(torch.cuda.FloatTensor)\n",
    "    #print(image.shape)\n",
    "    #print(type(image))\n",
    "    \n",
    "    # Returns a new tensor with a dimension of size one inserted at the specified position.\n",
    "    image = image.unsqueeze(0)\n",
    "    \n",
    "    output = model.forward(image)\n",
    "    \n",
    "    probabilities = torch.exp(output)\n",
    "    \n",
    "    # Probabilities and the indices of those probabilities corresponding to the classes\n",
    "    top_probabilities, top_indices = probabilities.topk(topk)\n",
    "    \n",
    "    # Convert to lists\n",
    "    top_probabilities = top_probabilities.detach().type(torch.FloatTensor).numpy().tolist()[0] \n",
    "    top_indices = top_indices.detach().type(torch.FloatTensor).numpy().tolist()[0] \n",
    "    \n",
    "    # Convert topk_indices to the actual class labels using class_to_idx\n",
    "    # Invert the dictionary so you get a mapping from index to class.\n",
    "    \n",
    "    idx_to_class = {value: key for key, value in model.class_to_idx.items()}\n",
    "    #print(idx_to_class)\n",
    "    \n",
    "    top_classes = [idx_to_class[index] for index in top_indices]\n",
    "    \n",
    "    return top_probabilities, top_classes\n",
    "    \n",
    "probs, classes = predict('flowers/test/15/image_06369.jpg', model)   \n",
    "print(probs)\n",
    "print(classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sanity Checking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display an image along with the top 5 classes\n",
    "\n",
    "# Plot flower input image\n",
    "plt.figure(figsize = (6,10))\n",
    "plot_1 = plt.subplot(2,1,1)\n",
    "\n",
    "#image = process_image('flowers/test/1/image_06743.jpg')\n",
    "image = process_image('flowers/test/15/image_06369.jpg')\n",
    "\n",
    "flower_title = flower_to_name['15']\n",
    "\n",
    "imshow(image, plot_1, title=flower_title);\n",
    "\n",
    "# Convert from the class integer encoding to actual flower names\n",
    "flower_names = [flower_to_name[i] for i in classes]\n",
    "\n",
    "# Plot the probabilities for the top 5 classes as a bar graph\n",
    "plt.subplot(2,1,2)\n",
    "\n",
    "sb.barplot(x=probs, y=flower_names, color=sb.color_palette()[0]);\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
